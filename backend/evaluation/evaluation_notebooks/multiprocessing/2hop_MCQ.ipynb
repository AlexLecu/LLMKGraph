{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422b213b-559d-4040-8641-bb634983a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from weaviate_rag.rag_system import GraphRAGSystem\n",
    "import ollama\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import Faithfulness, AnswerRelevancy, ContextRecall\n",
    "import re\n",
    "from ragas import EvaluationDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ec414d-18b7-44c1-b38f-2e27724ca342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your JSON data\n",
    "with open('/Users/alexlecu/PycharmProjects/LLMKGraph/backend/evaluation/data/grok_evaluation_datasets/2_Hop_MCQ_Questions.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "df = pd.DataFrame(data)\n",
    "df = df[['question', 'answer']]\n",
    "\n",
    "# Add columns for evaluation\n",
    "df['contexts'] = None  # Will be filled with retrieved passages\n",
    "df['with_context_answer'] = None  # Will be filled with RAG responses\n",
    "df['without_context_answer'] = None  # Will be filled with model-only responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2327c3e2-f39e-4013-90b8-41f0e4f6b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all questions in parallel\n",
    "from weaviate_rag.rag_utils_mp.evaluation_MCQ import process_question\n",
    "\n",
    "# Extract questions from DataFrame\n",
    "questions = df['question'].tolist()\n",
    "\n",
    "# Process questions in parallel\n",
    "print(\"Processing questions in parallel...\")\n",
    "with ProcessPoolExecutor(max_workers=8) as executor:\n",
    "    results = list(tqdm(executor.map(process_question, questions), total=len(questions)))\n",
    "\n",
    "# Unpack results into separate lists\n",
    "contexts, with_context_answers, without_context_answers = zip(*results)\n",
    "\n",
    "# Assign results to DataFrame\n",
    "df['contexts'] = contexts\n",
    "df['with_context_answer'] = with_context_answers\n",
    "df['without_context_answer'] = without_context_answers\n",
    "\n",
    "# Summary of processing\n",
    "error_count = sum(1 for c, w, wo in results if c is None)\n",
    "print(f\"Processed {len(results)} questions, with {error_count} errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e6469a-7750-412c-bd52-7f7e5bf0e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse responses to extract the selected multiple choice option\n",
    "def parse_mcq_answer(response):\n",
    "    \"\"\"\n",
    "    Extract the selected option (A, B, C, D, etc.) from an MCQ response,\n",
    "    handling various formats found in the dataset.\n",
    "    \"\"\"\n",
    "    if not response or not isinstance(response, str):\n",
    "        return None\n",
    "    \n",
    "    # First, normalize whitespace and convert to lowercase\n",
    "    response = response.lower().strip()\n",
    "    \n",
    "    # Check for standalone answer formats\n",
    "    if response in ['a', 'b', 'c', 'd']:\n",
    "        return response.upper()\n",
    "    \n",
    "    if response in ['a)', 'b)', 'c)', 'd)']:\n",
    "        return response[0].upper()\n",
    "    \n",
    "    # If the response starts with option followed by content (e.g., \"a) Yes\")\n",
    "    if re.match(r'^[a-d]\\)', response) or re.match(r'^[a-d]\\) ', response):\n",
    "        return response[0].upper()\n",
    "    \n",
    "    # Look for patterns like \"the best answer is A\"\n",
    "    best_answer_match = re.search(r'best answer is ([a-d])', response)\n",
    "    if best_answer_match:\n",
    "        return best_answer_match.group(1).upper()\n",
    "    \n",
    "    # Look for other common patterns\n",
    "    patterns = [\n",
    "        r'(?:the\\s+answer\\s+is\\s+)([a-d])',  # \"The answer is A\"\n",
    "        r'(?:option\\s+)([a-d])',              # \"Option A\"\n",
    "        r'(?:choice\\s+)([a-d])',              # \"Choice A\" \n",
    "        r'(?:select\\s+)([a-d])',              # \"Select A\"\n",
    "        r'(?:answer[:\\s]+)([a-d])',           # \"Answer: A\"\n",
    "        r'^([a-d])$',                         # Just \"A\" on a line\n",
    "        r'(?:answer\\s+is\\s+)([a-d])',         # \"Answer is A\"\n",
    "        r'(?:\\s+)([a-d])(?:\\s+is\\s+correct)', # \"A is correct\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, response)\n",
    "        if matches:\n",
    "            return matches[0].upper()\n",
    "            \n",
    "    # Check for capital letters A-D which are likely to be answers\n",
    "    capital_letters = re.findall(r'\\b([A-D])\\b', response)\n",
    "    if capital_letters:\n",
    "        return capital_letters[0]\n",
    "            \n",
    "    # If no match found with patterns, check for text like \" a \" or \" a.\" or \"a \"\n",
    "    for option in ['a', 'b', 'c', 'd']:\n",
    "        option_pattern = r'\\b{}\\b'.format(option)\n",
    "        if re.search(option_pattern, response):\n",
    "            return option.upper()\n",
    "    \n",
    "    # If still no match, check if there's any A, B, C, D in the text\n",
    "    for char in response:\n",
    "        if char.lower() in ['a', 'b', 'c', 'd']:\n",
    "            return char.upper()\n",
    "    \n",
    "    # No clear answer found\n",
    "    return None\n",
    "\n",
    "# Parse responses into MCQ answers\n",
    "df['with_context_parsed'] = df['with_context_answer'].apply(parse_mcq_answer)\n",
    "df['without_context_parsed'] = df['without_context_answer'].apply(parse_mcq_answer)\n",
    "\n",
    "# Convert reference answers to a standard format\n",
    "def standardize_reference_answer(answer):\n",
    "    \"\"\"\n",
    "    Standardize reference answers to a consistent format (just the letter A, B, C, or D)\n",
    "    \"\"\"\n",
    "    if not answer or not isinstance(answer, str):\n",
    "        return None\n",
    "        \n",
    "    answer = answer.strip().lower()\n",
    "    \n",
    "    # Handle \"A\", \"a\", \"A)\", \"a)\" formats\n",
    "    if answer in ['a', 'b', 'c', 'd']:\n",
    "        return answer.upper()\n",
    "    elif answer in ['a)', 'b)', 'c)', 'd)']:\n",
    "        return answer[0].upper()\n",
    "    \n",
    "    # If answer is \"a) Yes\", \"b) No\", etc.\n",
    "    if re.match(r'^[a-d]\\)', answer):\n",
    "        return answer[0].upper()\n",
    "    \n",
    "    # Try to extract any letter that might be the answer\n",
    "    for char in answer:\n",
    "        if char.lower() in ['a', 'b', 'c', 'd']:\n",
    "            return char.upper()\n",
    "            \n",
    "    return None\n",
    "\n",
    "# Standardize the reference answers\n",
    "df['standardized_answer'] = df['answer'].apply(standardize_reference_answer)\n",
    "\n",
    "# Display results\n",
    "print(\"Parsed Results:\")\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"Question {index+1}: {row['question']}\")\n",
    "    print(f\"Ground Truth: {row['standardized_answer']} (from '{row['answer']}')\")\n",
    "    print(f\"With Context: {row['with_context_parsed']}\")\n",
    "    print(f\"Without Context: {row['without_context_parsed']}\\n\")\n",
    "\n",
    "# Prepare for evaluation using standardized answers\n",
    "references = df['standardized_answer'].tolist()\n",
    "with_context_responses = df['with_context_parsed'].tolist()\n",
    "without_context_responses = df['without_context_parsed'].tolist()\n",
    "\n",
    "# Calculate accuracy\n",
    "with_context_correct = sum(1 for ref, pred in zip(references, with_context_responses) \n",
    "                           if ref == pred and ref is not None)\n",
    "without_context_correct = sum(1 for ref, pred in zip(references, without_context_responses) \n",
    "                             if ref == pred and ref is not None)\n",
    "\n",
    "# Count valid references (non-None)\n",
    "valid_references = sum(1 for ref in references if ref is not None)\n",
    "\n",
    "# Calculate accuracy (if there are valid references)\n",
    "if valid_references > 0:\n",
    "    with_context_accuracy = with_context_correct / valid_references\n",
    "    without_context_accuracy = without_context_correct / valid_references\n",
    "    \n",
    "    print(f\"With Context Accuracy: {with_context_accuracy:.2%} ({with_context_correct}/{valid_references})\")\n",
    "    print(f\"Without Context Accuracy: {without_context_accuracy:.2%} ({without_context_correct}/{valid_references})\")\n",
    "    print(f\"Improvement: {(with_context_accuracy - without_context_accuracy):.2%}\")\n",
    "else:\n",
    "    print(\"No valid references found for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6936f18c-bb6b-4e9d-b74f-b195c584232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare errors between with-context and without-context approaches\n",
    "print(\"\\nError Analysis:\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"{'Question':50} | {'Truth':5} | {'With Ctx':8} | {'Without Ctx':10} | {'Notes'}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Count different error types\n",
    "context_helped_count = 0\n",
    "context_misled_count = 0\n",
    "both_incorrect_count = 0\n",
    "both_correct_count = 0\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    # Get standardized answers for comparison\n",
    "    ground_truth = row['standardized_answer']\n",
    "    with_context = row['with_context_parsed']\n",
    "    without_context = row['without_context_parsed']\n",
    "    \n",
    "    # Check if either prediction is incorrect (using standardized answers)\n",
    "    with_context_correct = (with_context == ground_truth) if (with_context is not None and ground_truth is not None) else False\n",
    "    without_context_correct = (without_context == ground_truth) if (without_context is not None and ground_truth is not None) else False\n",
    "    \n",
    "    # Track counts for summary\n",
    "    if with_context_correct and without_context_correct:\n",
    "        both_correct_count += 1\n",
    "    elif with_context_correct and not without_context_correct:\n",
    "        context_helped_count += 1\n",
    "    elif not with_context_correct and without_context_correct:\n",
    "        context_misled_count += 1\n",
    "    elif not with_context_correct and not without_context_correct:\n",
    "        both_incorrect_count += 1\n",
    "    \n",
    "    # Only show questions where at least one approach was incorrect\n",
    "    if not (with_context_correct and without_context_correct) and ground_truth is not None:\n",
    "        # Truncate question for display\n",
    "        question = row['question'][:47] + \"...\" if len(row['question']) > 47 else row['question'].ljust(47)\n",
    "        \n",
    "        # Format for display\n",
    "        truth_display = str(ground_truth).ljust(5) if ground_truth else \"None\".ljust(5)\n",
    "        with_ctx_display = str(with_context).ljust(8) if with_context else \"None\".ljust(8)\n",
    "        without_ctx_display = str(without_context).ljust(10) if without_context else \"None\".ljust(10)\n",
    "        \n",
    "        # Determine notes\n",
    "        if with_context_correct and not without_context_correct:\n",
    "            notes = \"Context helped\"\n",
    "        elif not with_context_correct and without_context_correct:\n",
    "            notes = \"Context misled\"\n",
    "        else:\n",
    "            notes = \"Both incorrect\"\n",
    "            \n",
    "        print(f\"{question} | {truth_display} | {with_ctx_display} | {without_ctx_display} | {notes}\")\n",
    "\n",
    "# Print summary statistics\n",
    "total_questions = len(df)\n",
    "answerable_questions = sum(1 for ans in df['standardized_answer'] if ans is not None)\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"Total questions: {total_questions}\")\n",
    "print(f\"Answerable questions (with valid ground truth): {answerable_questions}\")\n",
    "print(f\"Both approaches correct: {both_correct_count} ({both_correct_count/answerable_questions*100:.1f}% of answerable)\")\n",
    "print(f\"Context helped: {context_helped_count} ({context_helped_count/answerable_questions*100:.1f}% of answerable)\")\n",
    "print(f\"Context misled: {context_misled_count} ({context_misled_count/answerable_questions*100:.1f}% of answerable)\")\n",
    "print(f\"Both approaches incorrect: {both_incorrect_count} ({both_incorrect_count/answerable_questions*100:.1f}% of answerable)\")\n",
    "\n",
    "# Net impact of context\n",
    "net_impact = context_helped_count - context_misled_count\n",
    "print(f\"\\nNet impact of context: {'+' if net_impact > 0 else ''}{net_impact} questions \" +\n",
    "      f\"({net_impact/answerable_questions*100:.1f}% of answerable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ceb96-a5e8-44a9-a1b9-16db0927a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the impact of context on each question\n",
    "df['context_impact'] = None\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    # Get standardized answers for comparison\n",
    "    ground_truth = row['standardized_answer']\n",
    "    with_context = row['with_context_parsed']\n",
    "    without_context = row['without_context_parsed']\n",
    "    \n",
    "    # Skip if ground truth or either prediction is None\n",
    "    if ground_truth is None or with_context is None or without_context is None:\n",
    "        df.at[i, 'context_impact'] = 'Unknown'\n",
    "        continue\n",
    "        \n",
    "    with_correct = with_context == ground_truth\n",
    "    without_correct = without_context == ground_truth\n",
    "    \n",
    "    if with_correct and not without_correct:\n",
    "        df.at[i, 'context_impact'] = 'Positive'\n",
    "    elif not with_correct and without_correct:\n",
    "        df.at[i, 'context_impact'] = 'Negative'\n",
    "    elif with_correct and without_correct:\n",
    "        df.at[i, 'context_impact'] = 'Neutral'\n",
    "    else:  # both incorrect\n",
    "        df.at[i, 'context_impact'] = 'No Help'\n",
    "\n",
    "# Summarize context impact\n",
    "impact_counts = df['context_impact'].value_counts()\n",
    "print(\"\\nContext Impact Summary:\")\n",
    "for impact, count in impact_counts.items():\n",
    "    percentage = count/len(df)*100\n",
    "    print(f\"{impact}: {count} questions ({percentage:.1f}%)\")\n",
    "\n",
    "# Calculate net positive impact\n",
    "positive_count = impact_counts.get('Positive', 0)\n",
    "negative_count = impact_counts.get('Negative', 0)\n",
    "net_impact = positive_count - negative_count\n",
    "net_percentage = net_impact / len(df) * 100\n",
    "print(f\"\\nNet Positive Impact: {net_impact} questions ({net_percentage:.1f}%)\")\n",
    "\n",
    "# Visualize context impact with improved colors\n",
    "plt.figure(figsize=(10, 6))\n",
    "impact_order = ['Positive', 'Neutral', 'Negative', 'No Help', 'Unknown']\n",
    "impact_colors = {\n",
    "    'Positive': '#60BD68',  # Green\n",
    "    'Neutral': '#5DA5DA',   # Blue\n",
    "    'Negative': '#F15854',  # Red\n",
    "    'No Help': '#FAA43A',   # Orange\n",
    "    'Unknown': '#CCCCCC'    # Gray\n",
    "}\n",
    "\n",
    "# Reorder the data for consistent display\n",
    "ordered_counts = []\n",
    "ordered_labels = []\n",
    "for impact in impact_order:\n",
    "    if impact in impact_counts:\n",
    "        ordered_counts.append(impact_counts[impact])\n",
    "        ordered_labels.append(impact)\n",
    "\n",
    "# Create the bar chart\n",
    "bars = plt.bar(ordered_labels, ordered_counts, color=[impact_colors[impact] for impact in ordered_labels])\n",
    "plt.title('Impact of Context on Multiple Choice Questions', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Impact Type', fontsize=12)\n",
    "plt.ylabel('Number of Questions', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add count and percentage labels on top of bars\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    percentage = height/len(df)*100\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "            f'{int(height)}\\n({percentage:.1f}%)',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional analysis: When does context help vs. hurt?\n",
    "# Create a function to extract key features from questions\n",
    "def extract_features(question):\n",
    "    features = {}\n",
    "    # Check for question types\n",
    "    features['is_cause_effect'] = 'cause' in question.lower() or 'because' in question.lower() or 'due to' in question.lower()\n",
    "    features['mentions_specific_condition'] = any(word in question.lower() for word in ['diabetes', 'hypertension', 'obesity', 'smoking'])\n",
    "    features['mentions_therapy'] = any(word in question.lower() for word in ['therapy', 'treatment', 'surgery', 'injection'])\n",
    "    features['about_disease_mechanism'] = 'mechanism' in question.lower() or 'pathway' in question.lower()\n",
    "    return features\n",
    "\n",
    "# Apply feature extraction\n",
    "for i, row in df.iterrows():\n",
    "    features = extract_features(row['question'])\n",
    "    for feature, value in features.items():\n",
    "        df.at[i, feature] = value\n",
    "\n",
    "# Analyze which features correlate with positive context impact\n",
    "print(\"\\nFeature Analysis for Context Impact:\")\n",
    "features = ['is_cause_effect', 'mentions_specific_condition', 'mentions_therapy', 'about_disease_mechanism']\n",
    "\n",
    "for feature in features:\n",
    "    positive_with_feature = df[(df['context_impact'] == 'Positive') & (df[feature] == True)].shape[0]\n",
    "    total_with_feature = df[df[feature] == True].shape[0]\n",
    "    \n",
    "    negative_with_feature = df[(df['context_impact'] == 'Negative') & (df[feature] == True)].shape[0]\n",
    "    \n",
    "    if total_with_feature > 0:\n",
    "        positive_rate = positive_with_feature / total_with_feature * 100\n",
    "        negative_rate = negative_with_feature / total_with_feature * 100\n",
    "        print(f\"\\nFeature: {feature}\")\n",
    "        print(f\"  Questions with this feature: {total_with_feature}\")\n",
    "        print(f\"  Positive impact rate: {positive_rate:.1f}%\")\n",
    "        print(f\"  Negative impact rate: {negative_rate:.1f}%\")\n",
    "        print(f\"  Net positive rate: {positive_rate - negative_rate:.1f}%\")\n",
    "\n",
    "# Visualize the feature analysis\n",
    "feature_data = []\n",
    "for feature in features:\n",
    "    positive_count = df[(df['context_impact'] == 'Positive') & (df[feature] == True)].shape[0]\n",
    "    negative_count = df[(df['context_impact'] == 'Negative') & (df[feature] == True)].shape[0]\n",
    "    total_count = df[df[feature] == True].shape[0]\n",
    "    \n",
    "    if total_count > 0:\n",
    "        feature_data.append({\n",
    "            'feature': feature.replace('_', ' ').title(),\n",
    "            'positive_rate': positive_count / total_count * 100,\n",
    "            'negative_rate': negative_count / total_count * 100,\n",
    "            'neutral_rate': (total_count - positive_count - negative_count) / total_count * 100\n",
    "        })\n",
    "\n",
    "if feature_data:\n",
    "    feature_df = pd.DataFrame(feature_data)\n",
    "    \n",
    "    # Plot the feature analysis\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    x = np.arange(len(feature_df))\n",
    "    width = 0.25\n",
    "    \n",
    "    plt.bar(x - width, feature_df['positive_rate'], width, label='Positive Impact', color='#60BD68')\n",
    "    plt.bar(x, feature_df['negative_rate'], width, label='Negative Impact', color='#F15854')\n",
    "    plt.bar(x + width, feature_df['neutral_rate'], width, label='Neutral/No Help', color='#5DA5DA')\n",
    "    \n",
    "    plt.xlabel('Question Features', fontsize=12)\n",
    "    plt.ylabel('Percentage of Questions', fontsize=12)\n",
    "    plt.title('Impact of Context by Question Feature', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(x, feature_df['feature'])\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc2c83-9070-4a4c-a3e9-08b149589f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output_2hop_MCQ.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
