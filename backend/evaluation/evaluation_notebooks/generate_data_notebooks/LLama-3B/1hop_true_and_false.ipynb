{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422b213b-559d-4040-8641-bb634983a3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from weaviate_rag.rag_system import GraphRAGSystem\n",
    "import ollama\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import Faithfulness, AnswerRelevancy, ContextRecall\n",
    "import re\n",
    "from ragas import EvaluationDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ec414d-18b7-44c1-b38f-2e27724ca342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your JSON data\n",
    "with open('/Users/alexlecu/PycharmProjects/LLMKGraph/backend/evaluation/data/grok_evaluation_datasets/1_Hop_True_or_False_Questions.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "df = pd.DataFrame(data)\n",
    "df = df[['question', 'answer']]\n",
    "\n",
    "# Add columns for evaluation\n",
    "df['contexts'] = None  # Will be filled with retrieved passages\n",
    "df['with_context_answer'] = None  # Will be filled with RAG responses\n",
    "df['without_context_answer'] = None  # Will be filled with model-only responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d604853-6ab1-4ed6-b3da-4a02eee6166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_true_false(response):\n",
    "    \"\"\"Parse the response to extract True/False answer\"\"\"\n",
    "    if response is None:\n",
    "        return None\n",
    "        \n",
    "    # Clean up response\n",
    "    response = response.strip().lower()\n",
    "    \n",
    "    # Check for True/False keywords\n",
    "    if \"true\" in response:\n",
    "        return True\n",
    "    elif \"false\" in response:\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"Warning: Could not parse true/false from: '{response}'\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27669b51-d110-4509-b0ed-146648e72766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all questions in parallel\n",
    "from weaviate_rag.rag_utils_mp.evaluation_true_and_false import process_question\n",
    "\n",
    "# Extract questions from DataFrame\n",
    "questions = df['question'].tolist()\n",
    "\n",
    "# Process questions in parallel\n",
    "print(\"Processing questions in parallel...\")\n",
    "with ProcessPoolExecutor(max_workers=8) as executor:\n",
    "    results = list(tqdm(executor.map(process_question, questions), total=len(questions)))\n",
    "\n",
    "# Unpack results into separate lists\n",
    "contexts, with_context_answers, without_context_answers = zip(*results)\n",
    "\n",
    "# Assign results to DataFrame\n",
    "df['contexts'] = contexts\n",
    "df['with_context_answer'] = with_context_answers\n",
    "df['without_context_answer'] = without_context_answers\n",
    "\n",
    "# Summary of processing\n",
    "error_count = sum(1 for c, w, wo in results if c is None)\n",
    "print(f\"Processed {len(results)} questions, with {error_count} errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e6469a-7750-412c-bd52-7f7e5bf0e5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse responses into boolean values\n",
    "df['with_context_parsed'] = df['with_context_answer'].apply(parse_true_false)\n",
    "df['without_context_parsed'] = df['without_context_answer'].apply(parse_true_false)\n",
    "\n",
    "# Display results\n",
    "print(\"Parsed Results:\")\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"Question {index+1}: {row['question']}\")\n",
    "    print(f\"Ground Truth: {row['answer']}\")\n",
    "    print(f\"With Context: {row['with_context_parsed']}\")\n",
    "    print(f\"Without Context: {row['without_context_parsed']}\\n\")\n",
    "\n",
    "# Prepare for evaluation\n",
    "references = df['answer'].tolist()\n",
    "with_context_responses = df['with_context_parsed'].tolist()\n",
    "without_context_responses = df['without_context_parsed'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a245f4-6883-4ff8-a888-ba8089685b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for with-context responses\n",
    "with_context_metrics = {}\n",
    "\n",
    "# Filter out None values\n",
    "valid_indices = [i for i, x in enumerate(with_context_responses) if x is not None]\n",
    "valid_refs = [references[i] for i in valid_indices]\n",
    "valid_preds = [with_context_responses[i] for i in valid_indices]\n",
    "\n",
    "if valid_preds:\n",
    "    with_context_metrics['accuracy'] = accuracy_score(valid_refs, valid_preds)\n",
    "    with_context_metrics['precision'] = precision_score(valid_refs, valid_preds)\n",
    "    with_context_metrics['recall'] = recall_score(valid_refs, valid_preds)\n",
    "    with_context_metrics['f1'] = f1_score(valid_refs, valid_preds)\n",
    "    with_context_metrics['confusion_matrix'] = confusion_matrix(valid_refs, valid_preds)\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(\"\\nWith Context - Evaluation Metrics:\")\n",
    "    print(f\"Accuracy: {with_context_metrics['accuracy']:.2f} ({with_context_metrics['accuracy'] * 100:.1f}%)\")\n",
    "    print(f\"Precision: {with_context_metrics['precision']:.2f}\")\n",
    "    print(f\"Recall: {with_context_metrics['recall']:.2f}\")\n",
    "    print(f\"F1-Score: {with_context_metrics['f1']:.2f}\")\n",
    "    print(f\"Confusion Matrix:\\n{with_context_metrics['confusion_matrix']}\")\n",
    "else:\n",
    "    print(\"No valid predictions to evaluate for with-context responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baab9213-f964-4c34-8751-3e21fde46357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for without-context responses\n",
    "without_context_metrics = {}\n",
    "\n",
    "# Filter out None values\n",
    "valid_indices = [i for i, x in enumerate(without_context_responses) if x is not None]\n",
    "valid_refs = [references[i] for i in valid_indices]\n",
    "valid_preds = [without_context_responses[i] for i in valid_indices]\n",
    "\n",
    "if valid_preds:\n",
    "    without_context_metrics['accuracy'] = accuracy_score(valid_refs, valid_preds)\n",
    "    without_context_metrics['precision'] = precision_score(valid_refs, valid_preds)\n",
    "    without_context_metrics['recall'] = recall_score(valid_refs, valid_preds)\n",
    "    without_context_metrics['f1'] = f1_score(valid_refs, valid_preds)\n",
    "    without_context_metrics['confusion_matrix'] = confusion_matrix(valid_refs, valid_preds)\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(\"\\nWithout Context - Evaluation Metrics:\")\n",
    "    print(f\"Accuracy: {without_context_metrics['accuracy']:.2f} ({without_context_metrics['accuracy'] * 100:.1f}%)\")\n",
    "    print(f\"Precision: {without_context_metrics['precision']:.2f}\")\n",
    "    print(f\"Recall: {without_context_metrics['recall']:.2f}\")\n",
    "    print(f\"F1-Score: {without_context_metrics['f1']:.2f}\")\n",
    "    print(f\"Confusion Matrix:\\n{without_context_metrics['confusion_matrix']}\")\n",
    "else:\n",
    "    print(\"No valid predictions to evaluate for without-context responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6936f18c-bb6b-4e9d-b74f-b195c584232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare errors between with-context and without-context approaches\n",
    "print(\"\\nError Analysis:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Question':50} | {'Ground Truth':12} | {'With Context':12} | {'Without Context':15} | {'Notes'}\") \n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    # Check if either prediction is incorrect\n",
    "    with_context_correct = row['with_context_parsed'] == row['answer'] if row['with_context_parsed'] is not None else False\n",
    "    without_context_correct = row['without_context_parsed'] == row['answer'] if row['without_context_parsed'] is not None else False\n",
    "    \n",
    "    # Only show questions where at least one approach was incorrect\n",
    "    if not (with_context_correct and without_context_correct):\n",
    "        question = row['question'][:47] + \"...\" if len(row['question']) > 50 else row['question'].ljust(50)\n",
    "        ground_truth = str(row['answer']).ljust(12)\n",
    "        with_context = str(row['with_context_parsed']).ljust(12)\n",
    "        without_context = str(row['without_context_parsed']).ljust(15)\n",
    "        \n",
    "        # Determine notes\n",
    "        if with_context_correct and not without_context_correct:\n",
    "            notes = \"Context helped\"\n",
    "        elif not with_context_correct and without_context_correct:\n",
    "            notes = \"Context misled\"\n",
    "        else:\n",
    "            notes = \"Both incorrect\"\n",
    "            \n",
    "        print(f\"{question} | {ground_truth} | {with_context} | {without_context} | {notes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea50826-2915-47ff-b9a9-c963c293638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics comparison\n",
    "if with_context_metrics and without_context_metrics:\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    with_context_values = [with_context_metrics[m] for m in metrics]\n",
    "    without_context_values = [without_context_metrics[m] for m in metrics]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    rects1 = ax.bar(x - width/2, with_context_values, width, label='With Context')\n",
    "    rects2 = ax.bar(x + width/2, without_context_values, width, label='Without Context')\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('Metrics')\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title('Performance Comparison: With vs. Without Context')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    \n",
    "    # Add value labels\n",
    "    def add_labels(rects):\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate(f'{height:.2f}',\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "    \n",
    "    add_labels(rects1)\n",
    "    add_labels(rects2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize confusion matrices\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # With context confusion matrix\n",
    "    sns.heatmap(with_context_metrics['confusion_matrix'], annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "                xticklabels=['Predicted False', 'Predicted True'],\n",
    "                yticklabels=['Actual False', 'Actual True'])\n",
    "    ax1.set_title('Confusion Matrix - With Context')\n",
    "    \n",
    "    # Without context confusion matrix\n",
    "    sns.heatmap(without_context_metrics['confusion_matrix'], annot=True, fmt='d', cmap='Blues', ax=ax2,\n",
    "                xticklabels=['Predicted False', 'Predicted True'],\n",
    "                yticklabels=['Actual False', 'Actual True'])\n",
    "    ax2.set_title('Confusion Matrix - Without Context')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough data to visualize comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2862bf-f6dc-4077-ace4-aead380082d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the impact of context on each question\n",
    "df['context_impact'] = None\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    # Skip if either prediction is None\n",
    "    if row['with_context_parsed'] is None or row['without_context_parsed'] is None:\n",
    "        df.at[i, 'context_impact'] = 'Unknown'\n",
    "        continue\n",
    "        \n",
    "    with_correct = row['with_context_parsed'] == row['answer']\n",
    "    without_correct = row['without_context_parsed'] == row['answer']\n",
    "    \n",
    "    if with_correct and not without_correct:\n",
    "        df.at[i, 'context_impact'] = 'Positive'\n",
    "    elif not with_correct and without_correct:\n",
    "        df.at[i, 'context_impact'] = 'Negative'\n",
    "    elif with_correct and without_correct:\n",
    "        df.at[i, 'context_impact'] = 'Neutral'\n",
    "    else:  # both incorrect\n",
    "        df.at[i, 'context_impact'] = 'No Help'\n",
    "\n",
    "# Summarize context impact\n",
    "impact_counts = df['context_impact'].value_counts()\n",
    "print(\"\\nContext Impact Summary:\")\n",
    "for impact, count in impact_counts.items():\n",
    "    print(f\"{impact}: {count} questions ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Visualize context impact\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = impact_counts.plot(kind='bar', color=['green', 'red', 'gray', 'orange'])\n",
    "plt.title('Impact of Context on True/False Questions')\n",
    "plt.xlabel('Impact Type')\n",
    "plt.ylabel('Number of Questions')\n",
    "\n",
    "# Add percentages on top of bars\n",
    "for i, v in enumerate(impact_counts):\n",
    "    ax.text(i, v + 0.1, f\"{v/len(df)*100:.1f}%\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ceb96-a5e8-44a9-a1b9-16db0927a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a summary report\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total Questions: {len(df)}\")\n",
    "print(f\"True Answers: {sum(df['answer'])}\")\n",
    "print(f\"False Answers: {len(df) - sum(df['answer'])}\")\n",
    "print(\"\\nPerformance Summary:\")\n",
    "\n",
    "if with_context_metrics and without_context_metrics:\n",
    "    print(f\"With Context Accuracy: {with_context_metrics['accuracy']:.2f} ({with_context_metrics['accuracy']*100:.1f}%)\")\n",
    "    print(f\"Without Context Accuracy: {without_context_metrics['accuracy']:.2f} ({without_context_metrics['accuracy']*100:.1f}%)\")\n",
    "    \n",
    "    # Determine if context helped overall\n",
    "    if with_context_metrics['accuracy'] > without_context_metrics['accuracy']:\n",
    "        diff = with_context_metrics['accuracy'] - without_context_metrics['accuracy']\n",
    "        print(f\"\\nContext improved accuracy by {diff:.2f} ({diff*100:.1f}%)\")\n",
    "    elif with_context_metrics['accuracy'] < without_context_metrics['accuracy']:\n",
    "        diff = without_context_metrics['accuracy'] - with_context_metrics['accuracy']\n",
    "        print(f\"\\nContext decreased accuracy by {diff:.2f} ({diff*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"\\nContext had no overall impact on accuracy\")\n",
    "\n",
    "# List questions where context helped\n",
    "helped_questions = df[df['context_impact'] == 'Positive']\n",
    "if not helped_questions.empty:\n",
    "    print(\"\\nQuestions where context helped:\")\n",
    "    for i, row in helped_questions.iterrows():\n",
    "        print(f\"- {row['question']}\")\n",
    "\n",
    "# List questions where context was misleading\n",
    "misled_questions = df[df['context_impact'] == 'Negative']\n",
    "if not misled_questions.empty:\n",
    "    print(\"\\nQuestions where context was misleading:\")\n",
    "    for i, row in misled_questions.iterrows():\n",
    "        print(f\"- {row['question']}\")\n",
    "\n",
    "print(\"\\nConclusions:\")\n",
    "print(\"1. Context retrieval\" + (\" improved \" if with_context_metrics.get('accuracy', 0) > without_context_metrics.get('accuracy', 0) else \" did not improve \") + \"overall accuracy for true/false questions.\")\n",
    "print(f\"2. {impact_counts.get('Positive', 0)} questions benefited from context, while {impact_counts.get('Negative', 0)} questions were negatively affected.\")\n",
    "print(\"3. Recommendations:\")\n",
    "print(\"   - \" + (\"Continue using RAG for true/false questions\" if with_context_metrics.get('accuracy', 0) > without_context_metrics.get('accuracy', 0) else \"Improve context retrieval or consider limiting RAG for true/false questions\"))\n",
    "print(\"   - Consider a hybrid approach that evaluates context quality before deciding whether to use it\")\n",
    "print(\"   - Expand the evaluation dataset to include more diverse true/false questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6345d4e4-4a8e-4c14-a998-98afba006a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output_1hop_true_and_false.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb81c866-971d-4b44-aa97-9c604b0f3bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
