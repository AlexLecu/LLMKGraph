{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21dd375-5b6b-468d-b0fd-ef1606bdbe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from together import Together\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize Together client\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.3-70B-Instruct-Turbo\"\n",
    "TOGETHER_API_KEY = os.environ[\"TOGETHER_API_KEY\"]\n",
    "\n",
    "client = Together(api_key=TOGETHER_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3cd774-a1ec-480b-a3d8-c7d8f32664a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/alexlecu/PycharmProjects/LLMKGraph/backend/evaluation/data/grok_evaluation_datasets/1_Hop_True_or_False_Questions.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "df = pd.DataFrame(data)\n",
    "df = df[['question', 'answer']]\n",
    "\n",
    "# Add columns for evaluation\n",
    "df['contexts'] = None  # Will be filled with retrieved passages\n",
    "df['with_context_answer'] = None  # Will be filled with RAG responses\n",
    "df['without_context_answer'] = None  # Will be filled with model-only responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b69333-1be7-48ba-b87b-5bc3a39499b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate_rag.rag_system import GraphRAGSystem\n",
    "\n",
    "# Configure for Together AI\n",
    "model_config = {\n",
    "    'provider': 'together',\n",
    "    'model_name': MODEL_NAME,\n",
    "    'api_key': TOGETHER_API_KEY\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7975b5e1-372c-44f7-9e01-c64b478f18e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_with_context_together(question, context, model=MODEL_NAME):\n",
    "    user_prompt = f\"\"\"Given the following medical information, determine if the statement below is True or False.\n",
    "\n",
    "    MEDICAL INFORMATION:\n",
    "    {context}\n",
    "    \n",
    "    STATEMENT TO VERIFY:\n",
    "    {question}\n",
    "    \n",
    "    IMPORTANT: Respond with ONLY the word \"True\" or \"False\".\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a medical knowledge assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            max_tokens=10\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Together API: {e}\")\n",
    "        time.sleep(1)  # Rate limiting\n",
    "        return None\n",
    "\n",
    "def generate_answer_without_context_together(question, model=MODEL_NAME):\n",
    "    \"\"\"Generate answer using Together AI without context\"\"\"\n",
    "    user_prompt = f\"\"\"\n",
    "    Determine if this medical statement is TRUE or FALSE based on your knowledge.\n",
    "    \n",
    "    STATEMENT TO VERIFY:\n",
    "    {question}\n",
    "    \n",
    "    IMPORTANT: Respond with ONLY the word \"True\" or \"False\".\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a medical knowledge assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            max_tokens=10\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Together API: {e}\")\n",
    "        time.sleep(1)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955a538-f988-430d-90cb-c2560d7d1b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_true_false(response):\n",
    "    \"\"\"Parse the response to extract True/False answer\"\"\"\n",
    "    if response is None:\n",
    "        return None\n",
    "        \n",
    "    # Clean up response\n",
    "    response = response.strip().lower()\n",
    "    \n",
    "    # Check for True/False keywords\n",
    "    if \"true\" in response:\n",
    "        return True\n",
    "    elif \"false\" in response:\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"Warning: Could not parse true/false from: '{response}'\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184d026b-d7e1-4c25-9fe3-320ca5b41f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\"):\n",
    "    question = row['question']\n",
    "    \n",
    "    try:\n",
    "        # Generate context using Together AI (end-to-end evaluation)\n",
    "        analyzer = GraphRAGSystem(question, model_config)\n",
    "        context = analyzer.analyze()\n",
    "        \n",
    "        # Generate answers\n",
    "        with_context = generate_answer_with_context_together(question, context)\n",
    "        without_context = generate_answer_without_context_together(question)\n",
    "               \n",
    "        df.at[idx, 'contexts'] = context\n",
    "        df.at[idx, 'with_context_answer'] = with_context\n",
    "        df.at[idx, 'without_context_answer'] = without_context\n",
    "        \n",
    "        # Rate limiting\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error on question {idx}: {e}\")\n",
    "        df.at[idx, 'contexts'] = None\n",
    "        df.at[idx, 'with_context_answer'] = None\n",
    "        df.at[idx, 'without_context_answer'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ac4eb9-6ec1-4004-ba60-2fd9c6644e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse responses into boolean values\n",
    "df['with_context_parsed'] = df['with_context_answer'].apply(parse_true_false)\n",
    "df['without_context_parsed'] = df['without_context_answer'].apply(parse_true_false)\n",
    "\n",
    "# Display results\n",
    "print(\"Parsed Results:\")\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"Question {index+1}: {row['question']}\")\n",
    "    print(f\"Ground Truth: {row['answer']}\")\n",
    "    print(f\"With Context: {row['with_context_parsed']}\")\n",
    "    print(f\"Without Context: {row['without_context_parsed']}\\n\")\n",
    "\n",
    "# Prepare for evaluation\n",
    "references = df['answer'].tolist()\n",
    "with_context_responses = df['with_context_parsed'].tolist()\n",
    "without_context_responses = df['without_context_parsed'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2d408d-bf4a-4644-9f0b-65a2af3c02e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for with-context responses\n",
    "with_context_metrics = {}\n",
    "\n",
    "# Filter out None values\n",
    "valid_indices = [i for i, x in enumerate(with_context_responses) if x is not None]\n",
    "valid_refs = [references[i] for i in valid_indices]\n",
    "valid_preds = [with_context_responses[i] for i in valid_indices]\n",
    "\n",
    "if valid_preds:\n",
    "    with_context_metrics['accuracy'] = accuracy_score(valid_refs, valid_preds)\n",
    "    with_context_metrics['precision'] = precision_score(valid_refs, valid_preds)\n",
    "    with_context_metrics['recall'] = recall_score(valid_refs, valid_preds)\n",
    "    with_context_metrics['f1'] = f1_score(valid_refs, valid_preds)\n",
    "    with_context_metrics['confusion_matrix'] = confusion_matrix(valid_refs, valid_preds)\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(\"\\nWith Context - Evaluation Metrics:\")\n",
    "    print(f\"Accuracy: {with_context_metrics['accuracy']:.2f} ({with_context_metrics['accuracy'] * 100:.1f}%)\")\n",
    "    print(f\"Precision: {with_context_metrics['precision']:.2f}\")\n",
    "    print(f\"Recall: {with_context_metrics['recall']:.2f}\")\n",
    "    print(f\"F1-Score: {with_context_metrics['f1']:.2f}\")\n",
    "    print(f\"Confusion Matrix:\\n{with_context_metrics['confusion_matrix']}\")\n",
    "else:\n",
    "    print(\"No valid predictions to evaluate for with-context responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4170c1f3-5f06-4932-b3bc-1d2b757cd75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for without-context responses\n",
    "without_context_metrics = {}\n",
    "\n",
    "# Filter out None values\n",
    "valid_indices = [i for i, x in enumerate(without_context_responses) if x is not None]\n",
    "valid_refs = [references[i] for i in valid_indices]\n",
    "valid_preds = [without_context_responses[i] for i in valid_indices]\n",
    "\n",
    "if valid_preds:\n",
    "    without_context_metrics['accuracy'] = accuracy_score(valid_refs, valid_preds)\n",
    "    without_context_metrics['precision'] = precision_score(valid_refs, valid_preds)\n",
    "    without_context_metrics['recall'] = recall_score(valid_refs, valid_preds)\n",
    "    without_context_metrics['f1'] = f1_score(valid_refs, valid_preds)\n",
    "    without_context_metrics['confusion_matrix'] = confusion_matrix(valid_refs, valid_preds)\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(\"\\nWithout Context - Evaluation Metrics:\")\n",
    "    print(f\"Accuracy: {without_context_metrics['accuracy']:.2f} ({without_context_metrics['accuracy'] * 100:.1f}%)\")\n",
    "    print(f\"Precision: {without_context_metrics['precision']:.2f}\")\n",
    "    print(f\"Recall: {without_context_metrics['recall']:.2f}\")\n",
    "    print(f\"F1-Score: {without_context_metrics['f1']:.2f}\")\n",
    "    print(f\"Confusion Matrix:\\n{without_context_metrics['confusion_matrix']}\")\n",
    "else:\n",
    "    print(\"No valid predictions to evaluate for without-context responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f98142e-9380-4484-8807-1cea5410242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare errors between with-context and without-context approaches\n",
    "print(\"\\nError Analysis:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Question':50} | {'Ground Truth':12} | {'With Context':12} | {'Without Context':15} | {'Notes'}\") \n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    # Check if either prediction is incorrect\n",
    "    with_context_correct = row['with_context_parsed'] == row['answer'] if row['with_context_parsed'] is not None else False\n",
    "    without_context_correct = row['without_context_parsed'] == row['answer'] if row['without_context_parsed'] is not None else False\n",
    "    \n",
    "    # Only show questions where at least one approach was incorrect\n",
    "    if not (with_context_correct and without_context_correct):\n",
    "        question = row['question'][:47] + \"...\" if len(row['question']) > 50 else row['question'].ljust(50)\n",
    "        ground_truth = str(row['answer']).ljust(12)\n",
    "        with_context = str(row['with_context_parsed']).ljust(12)\n",
    "        without_context = str(row['without_context_parsed']).ljust(15)\n",
    "        \n",
    "        # Determine notes\n",
    "        if with_context_correct and not without_context_correct:\n",
    "            notes = \"Context helped\"\n",
    "        elif not with_context_correct and without_context_correct:\n",
    "            notes = \"Context misled\"\n",
    "        else:\n",
    "            notes = \"Both incorrect\"\n",
    "            \n",
    "        print(f\"{question} | {ground_truth} | {with_context} | {without_context} | {notes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb7f098-1b1b-4251-a53a-5edbd7f284fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics comparison\n",
    "if with_context_metrics and without_context_metrics:\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    with_context_values = [with_context_metrics[m] for m in metrics]\n",
    "    without_context_values = [without_context_metrics[m] for m in metrics]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    rects1 = ax.bar(x - width/2, with_context_values, width, label='With Context')\n",
    "    rects2 = ax.bar(x + width/2, without_context_values, width, label='Without Context')\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('Metrics')\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title('Performance Comparison: With vs. Without Context')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    \n",
    "    # Add value labels\n",
    "    def add_labels(rects):\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate(f'{height:.2f}',\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom')\n",
    "    \n",
    "    add_labels(rects1)\n",
    "    add_labels(rects2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize confusion matrices\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # With context confusion matrix\n",
    "    sns.heatmap(with_context_metrics['confusion_matrix'], annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "                xticklabels=['Predicted False', 'Predicted True'],\n",
    "                yticklabels=['Actual False', 'Actual True'])\n",
    "    ax1.set_title('Confusion Matrix - With Context')\n",
    "    \n",
    "    # Without context confusion matrix\n",
    "    sns.heatmap(without_context_metrics['confusion_matrix'], annot=True, fmt='d', cmap='Blues', ax=ax2,\n",
    "                xticklabels=['Predicted False', 'Predicted True'],\n",
    "                yticklabels=['Actual False', 'Actual True'])\n",
    "    ax2.set_title('Confusion Matrix - Without Context')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough data to visualize comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc534bc7-cb6c-411d-a20b-ae90a0f5e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the impact of context on each question\n",
    "df['context_impact'] = None\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    # Skip if either prediction is None\n",
    "    if row['with_context_parsed'] is None or row['without_context_parsed'] is None:\n",
    "        df.at[i, 'context_impact'] = 'Unknown'\n",
    "        continue\n",
    "        \n",
    "    with_correct = row['with_context_parsed'] == row['answer']\n",
    "    without_correct = row['without_context_parsed'] == row['answer']\n",
    "    \n",
    "    if with_correct and not without_correct:\n",
    "        df.at[i, 'context_impact'] = 'Positive'\n",
    "    elif not with_correct and without_correct:\n",
    "        df.at[i, 'context_impact'] = 'Negative'\n",
    "    elif with_correct and without_correct:\n",
    "        df.at[i, 'context_impact'] = 'Neutral'\n",
    "    else:  # both incorrect\n",
    "        df.at[i, 'context_impact'] = 'No Help'\n",
    "\n",
    "# Summarize context impact\n",
    "impact_counts = df['context_impact'].value_counts()\n",
    "print(\"\\nContext Impact Summary:\")\n",
    "for impact, count in impact_counts.items():\n",
    "    print(f\"{impact}: {count} questions ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Visualize context impact\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = impact_counts.plot(kind='bar', color=['green', 'red', 'gray', 'orange'])\n",
    "plt.title('Impact of Context on True/False Questions')\n",
    "plt.xlabel('Impact Type')\n",
    "plt.ylabel('Number of Questions')\n",
    "\n",
    "# Add percentages on top of bars\n",
    "for i, v in enumerate(impact_counts):\n",
    "    ax.text(i, v + 0.1, f\"{v/len(df)*100:.1f}%\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d627928-9614-4363-9851-14729839bd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a summary report\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total Questions: {len(df)}\")\n",
    "print(f\"True Answers: {sum(df['answer'])}\")\n",
    "print(f\"False Answers: {len(df) - sum(df['answer'])}\")\n",
    "print(\"\\nPerformance Summary:\")\n",
    "\n",
    "if with_context_metrics and without_context_metrics:\n",
    "    print(f\"With Context Accuracy: {with_context_metrics['accuracy']:.2f} ({with_context_metrics['accuracy']*100:.1f}%)\")\n",
    "    print(f\"Without Context Accuracy: {without_context_metrics['accuracy']:.2f} ({without_context_metrics['accuracy']*100:.1f}%)\")\n",
    "    \n",
    "    # Determine if context helped overall\n",
    "    if with_context_metrics['accuracy'] > without_context_metrics['accuracy']:\n",
    "        diff = with_context_metrics['accuracy'] - without_context_metrics['accuracy']\n",
    "        print(f\"\\nContext improved accuracy by {diff:.2f} ({diff*100:.1f}%)\")\n",
    "    elif with_context_metrics['accuracy'] < without_context_metrics['accuracy']:\n",
    "        diff = without_context_metrics['accuracy'] - with_context_metrics['accuracy']\n",
    "        print(f\"\\nContext decreased accuracy by {diff:.2f} ({diff*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"\\nContext had no overall impact on accuracy\")\n",
    "\n",
    "# List questions where context helped\n",
    "helped_questions = df[df['context_impact'] == 'Positive']\n",
    "if not helped_questions.empty:\n",
    "    print(\"\\nQuestions where context helped:\")\n",
    "    for i, row in helped_questions.iterrows():\n",
    "        print(f\"- {row['question']}\")\n",
    "\n",
    "# List questions where context was misleading\n",
    "misled_questions = df[df['context_impact'] == 'Negative']\n",
    "if not misled_questions.empty:\n",
    "    print(\"\\nQuestions where context was misleading:\")\n",
    "    for i, row in misled_questions.iterrows():\n",
    "        print(f\"- {row['question']}\")\n",
    "\n",
    "print(\"\\nConclusions:\")\n",
    "print(\"1. Context retrieval\" + (\" improved \" if with_context_metrics.get('accuracy', 0) > without_context_metrics.get('accuracy', 0) else \" did not improve \") + \"overall accuracy for true/false questions.\")\n",
    "print(f\"2. {impact_counts.get('Positive', 0)} questions benefited from context, while {impact_counts.get('Negative', 0)} questions were negatively affected.\")\n",
    "print(\"3. Recommendations:\")\n",
    "print(\"   - \" + (\"Continue using RAG for true/false questions\" if with_context_metrics.get('accuracy', 0) > without_context_metrics.get('accuracy', 0) else \"Improve context retrieval or consider limiting RAG for true/false questions\"))\n",
    "print(\"   - Consider a hybrid approach that evaluates context quality before deciding whether to use it\")\n",
    "print(\"   - Expand the evaluation dataset to include more diverse true/false questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab85a0ae-90d8-4948-99b4-40951b84208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output_llama70b_1hop_true_and_false.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
